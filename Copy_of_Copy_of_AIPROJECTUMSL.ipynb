{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOYPJm1M5OYlj/aBufrSMTY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NAGARAJVODNALA/AIFINALPROJECT/blob/main/Copy_of_Copy_of_AIPROJECTUMSL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **PHASE 1**"
      ],
      "metadata": {
        "id": "vJwkeNnLJnjL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#IMPORT LIBRARIES\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "import missingno as msno\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "\n"
      ],
      "metadata": {
        "id": "qcp-PH66NpQh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PHASE 1\n",
        "\n",
        "# A) Load the dataset\n",
        "data = pd.read_csv('HTRU_2_AIDATASET.csv')\n"
      ],
      "metadata": {
        "id": "bBD0rDwZNqPK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# B) Show number of rows and columns\n",
        "print(f\"The dataset HTRU_2_AIDATASET has {data.shape[0]} rows and {data.shape[1]} columns.\")\n"
      ],
      "metadata": {
        "id": "Z79CaHN7NqDO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# C)-----Check for missing values\n",
        "missing_values = data.isnull().sum()\n",
        "print(\"\\nMissing values in each column:\")\n",
        "print(missing_values)\n"
      ],
      "metadata": {
        "id": "7wO-OQXdNwBY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# D)Shows first five rows of the dataset\n",
        "data.head()\n"
      ],
      "metadata": {
        "id": "X4gEZXdYNv4C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# E)Let's obtain a brief overview of the dataframe\n",
        "data.info()\n"
      ],
      "metadata": {
        "id": "eBG8HfuYNvwF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# F)Shows number of rows in each column with bar diagram\n",
        "msno.bar(data)\n"
      ],
      "metadata": {
        "id": "_BDVM3rGN1ir"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# G) Distribute each column , split the input features and output parameters\n",
        "X = data.iloc[:, :-1].values\n",
        "y = data.iloc[:, -1].values\n"
      ],
      "metadata": {
        "id": "3YXa1i_dN3Ke"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# H)Show distribution of each column\n",
        "#----- Histograms for input features\n",
        "input_features = data.columns[:-1]\n",
        "data[input_features].hist(bins=30, figsize=(15, 10))\n",
        "plt.suptitle(\"Histograms of Input Features\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "VDzBEWTLN66a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# I)Pie chart for 'class' column\n",
        "class_counts = data['Class'].value_counts()\n",
        "plt.pie(class_counts, labels=['0', '1'], autopct='%1.1f%%')\n",
        "plt.title(\"Class Distribution\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "aHRFTvnKONmB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# J)Unique values of the dtatset\n",
        "data.nunique()\n"
      ],
      "metadata": {
        "id": "7For-kx3OJek"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MH689VYQmp8K"
      },
      "outputs": [],
      "source": [
        "# K) Normalize the data\n",
        "#X = (X - np.min(X, axis=0)) / (np.max(X, axis=0) - np.min(X, axis=0))\n",
        "\n",
        "data_normalized = (data.iloc[:, :-1] - data.iloc[:, :-1].min()) / (data.iloc[:, :-1].max() - data.iloc[:, :-1].min())\n",
        "data_normalized['Class'] = data.iloc[:, -1]\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **PHASE 2**"
      ],
      "metadata": {
        "id": "z53b69yfJ3XW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# PHASE 2\n",
        "\n",
        "# L)NOW BUILD A MODEL WITH SINGLE LAYER AND SINGLE NEURON\n",
        "\n",
        "# PART 1: Single layer, single neuron model\n",
        "single_layer_model = Sequential()\n",
        "single_layer_model.add(Dense(1, activation='sigmoid', input_dim=8))\n",
        "single_layer_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# M)Train the data using Keras, TensorFlow\n",
        "history_single_layer = single_layer_model.fit(X, y, epochs=20, batch_size=32, verbose=0)\n",
        "\n",
        "\n",
        "# N)Print the accuracy\n",
        "_, accuracy_single_layer = single_layer_model.evaluate(X, y, verbose=0)\n",
        "print('Single Layer Model Accuracy: %.2f' % (accuracy_single_layer * 100))\n",
        "\n"
      ],
      "metadata": {
        "id": "NUfGodSLOBP8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# O)BUILD A MULTILAYER MODEL WITH MORE NEURONS\n",
        "\n",
        "# PART 2: Multilayer model with more layers and more neurons\n",
        "multi_layer_model = Sequential()\n",
        "multi_layer_model.add(Dense(16, activation='relu', input_dim=8))\n",
        "multi_layer_model.add(Dense(8, activation='relu'))\n",
        "multi_layer_model.add(Dense(1, activation='sigmoid'))\n",
        "multi_layer_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# P)Train the data using Keras, TensorFlow\n",
        "history_multi_layer = multi_layer_model.fit(X, y, epochs=20, batch_size=32, verbose=0)\n",
        "\n",
        "# Q)Print the accuracy\n",
        "_, accuracy_multi_layer = multi_layer_model.evaluate(X, y, verbose=0)\n",
        "print('Multilayer Model Accuracy: %.2f' % (accuracy_multi_layer * 100))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "X8IjkvhIJ2N6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# O)BUILD A MULTILAYER MODEL WITH MORE  NEURONS THAN PREVIOUS PART 3\n",
        "\n",
        "# PART 3: Multilayer model with more layers and more neurons\n",
        "\n",
        "multi_layer_model2 = Sequential()\n",
        "multi_layer_model2.add(Dense(128, activation='relu', input_dim=8))\n",
        "multi_layer_model2.add(Dense(64, activation='relu'))\n",
        "multi_layer_model2.add(Dense(32, activation='relu'))\n",
        "multi_layer_model2.add(Dense(16, activation='relu'))\n",
        "multi_layer_model2.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "multi_layer_model2.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# P)Train the data using Keras, TensorFlow\n",
        "history_multi_layer2 = multi_layer_model2.fit(X, y, epochs=20, batch_size=32, verbose=0)\n",
        "\n",
        "# Q)Print the accuracy\n",
        "_, accuracy_multi_layer2 = multi_layer_model2.evaluate(X, y, verbose=0)\n",
        "print('Multilayer Model Accuracy: %.2f' % (accuracy_multi_layer * 100))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2Hcg-Q1xO7Pn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# O)BUILD A MULTILAYER MODEL WITH MORE NEURONS -------------- \n",
        "\n",
        "# PART 4: Multilayer model with more layers and more neurons\n",
        "\n",
        "multi_layer_model3 = Sequential()\n",
        "multi_layer_model3.add(Dense(256, activation='relu', input_dim=8))\n",
        "multi_layer_model3.add(Dense(128, activation='relu'))\n",
        "multi_layer_model3.add(Dense(64, activation='relu'))\n",
        "multi_layer_model3.add(Dense(32, activation='relu'))\n",
        "multi_layer_model3.add(Dense(1, activation='sigmoid'))\n",
        "multi_layer_model3.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# P)Train the data using Keras, TensorFlow\n",
        "history_multi_layer3 = multi_layer_model3.fit(X, y, epochs=20, batch_size=32, verbose=0)\n",
        "\n",
        "# Q)Print the accuracy\n",
        "_, accuracy_multi_layer3 = multi_layer_model3.evaluate(X, y, verbose=0)\n",
        "print('Multilayer Model Accuracy 2: %.2f' % (accuracy_multi_layer * 100))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "t6HxfAMeOc3r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PHASE 2 - Additional tasks\n",
        "\n",
        "# R) Add output as an additional input feature\n",
        "X_extended = np.column_stack((X, y))\n",
        "\n",
        "# S) Define a function that represents the model\n",
        "def predict(model, input_data):\n",
        "    return model.predict(input_data)\n",
        "\n",
        "# T) Build a multilayer model with output as an additional input feature\n",
        "multi_layer_model_ext = Sequential()\n",
        "multi_layer_model_ext.add(Dense(128, activation='relu', input_dim=9))\n",
        "multi_layer_model_ext.add(Dense(64, activation='relu'))\n",
        "multi_layer_model_ext.add(Dense(32, activation='relu'))\n",
        "multi_layer_model_ext.add(Dense(16, activation='relu'))\n",
        "multi_layer_model_ext.add(Dense(1, activation='sigmoid'))\n",
        "multi_layer_model_ext.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# U) Train the model with the extended input data\n",
        "history_multi_layer_ext = multi_layer_model_ext.fit(X_extended, y, epochs=20, batch_size=32, verbose=0)\n",
        "\n",
        "# V) Evaluate the accuracy\n",
        "_, accuracy_multi_layer_ext = multi_layer_model_ext.evaluate(X_extended, y, verbose=0)\n",
        "print('Multilayer Model with Extended Input Accuracy: %.2f' % (accuracy_multi_layer_ext * 100))\n",
        "\n",
        "# W) Create a function that serves as a prediction model\n",
        "def prediction_model(model, input_data):\n",
        "    return predict(model, input_data)\n",
        "\n",
        "# X) Test the prediction function with an example\n",
        "test_input = X_extended[0]  # Take the first row of the extended input data as an example\n",
        "prediction = prediction_model(multi_layer_model_ext, test_input.reshape(1, -1))\n",
        "print(f'Prediction for the test input: {prediction}')\n"
      ],
      "metadata": {
        "id": "B9VaJ1TdDZT9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Y)Import required libraries\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Z)Scale the data using StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# A1)Create and train a logistic regression model with increased max_iter\n",
        "log_reg_model = LogisticRegression(max_iter=1000)\n",
        "log_reg_model.fit(X_scaled, y)\n",
        "\n",
        "# B1)Make predictions on the entire dataset\n",
        "y_pred = log_reg_model.predict(X_scaled)\n",
        "\n",
        "# C1)Calculate the accuracy\n",
        "log_reg_accuracy = accuracy_score(y, y_pred)\n",
        "print('Logistic Regression Model Accuracy: %.2f' % (log_reg_accuracy * 100))\n"
      ],
      "metadata": {
        "id": "8FTZu0p6XrM3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# D1)Import required libraries\n",
        "from sklearn.dummy import DummyClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# E1)Create and train a random baseline classifier on the entire dataset\n",
        "random_baseline_model = DummyClassifier(strategy='stratified', random_state=42)\n",
        "random_baseline_model.fit(X, y)\n",
        "\n",
        "# F1)Make predictions on the entire dataset\n",
        "y_pred = random_baseline_model.predict(X)\n",
        "\n",
        "# G1)Calculate the accuracy\n",
        "random_baseline_accuracy = accuracy_score(y, y_pred)\n",
        "print('Random Baseline Classifier Accuracy: %.2f' % (random_baseline_accuracy * 100))\n"
      ],
      "metadata": {
        "id": "PHvLOX2laaET"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# H1)Import additional library for plotting\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# I1)Train the single layer model for 20 epochs\n",
        "history_single_layer = single_layer_model.fit(X, y, epochs=20, batch_size=32, verbose=0)\n",
        "\n",
        "# J1)Train the multilayer model for 20 epochs\n",
        "history_multi_layer = multi_layer_model.fit(X, y, epochs=20, batch_size=32, verbose=0)\n",
        "history_multi_layer2 = multi_layer_model2.fit(X, y, epochs=20, batch_size=32, verbose=0)\n",
        "history_multi_layer3 = multi_layer_model3.fit(X, y, epochs=20, batch_size=32, verbose=0)\n",
        "\n",
        "# K1)Train the multilayer model with extended input for 20 epochs\n",
        "history_multi_layer_ext = multi_layer_model_ext.fit(X_extended, y, epochs=20, batch_size=32, verbose=0)\n",
        "\n",
        "# L1)Extract the accuracies\n",
        "acc_single_layer = history_single_layer.history['accuracy']\n",
        "acc_multi_layer = history_multi_layer.history['accuracy']\n",
        "acc_multi_layer2= history_multi_layer2.history['accuracy']\n",
        "acc_multi_layer3 = history_multi_layer3.history['accuracy']\n",
        "acc_multi_layer_ext = history_multi_layer_ext.history['accuracy']\n",
        "\n",
        "# M1)Create an array with the epoch numbers\n",
        "epochs = range(1, 11)\n",
        "\n",
        "# N1)Plot the accuracies\n",
        "plt.plot(epochs, acc_single_layer, label='Single Layer Model')\n",
        "plt.plot(epochs, acc_multi_layer, label='Multilayer Model')\n",
        "plt.plot(epochs, acc_multi_layer2, label='Multilayer Model2')\n",
        "plt.plot(epochs, acc_multi_layer3, label='Multilayer Model3')\n",
        "plt.plot(epochs, acc_multi_layer_ext, label='Multilayer Model with Extended Input')\n",
        "plt.axhline(log_reg_accuracy, color='red', linestyle='--', label='Logistic Regression')\n",
        "plt.axhline(random_baseline_accuracy, color='pink', linestyle='dotted', label='Random Baseline Accuracy')\n",
        "\n",
        "# O1)Add title and labels for axes\n",
        "plt.title('Model Accuracies')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "\n",
        "# P1)Add a legend\n",
        "plt.legend()\n",
        "\n",
        "# Q1)Display the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "gsuqKnp_Pxj0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**PHASE 3**"
      ],
      "metadata": {
        "id": "uotRPm72JhYR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# R1)Shuffle data\n",
        "shuffled_indices = np.random.permutation(len(X))\n",
        "X_shuffled = X[shuffled_indices]\n",
        "y_shuffled = y[shuffled_indices]"
      ],
      "metadata": {
        "id": "_7Gh5fohPTSY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# S1) Split data, test, and validation\n",
        "split_ratio = 0.8\n",
        "split_index = int(len(X) * split_ratio)\n",
        "X_train, X_val = X_shuffled[:split_index], X_shuffled[split_index:]\n",
        "y_train, y_val = y_shuffled[:split_index], y_shuffled[split_index:]\n",
        "\n",
        "\n",
        "# Print training and validation data\n",
        "print(\"Training data (X_train):\")\n",
        "print(X_train)\n",
        "print(\"\\nTraining labels (y_train):\")\n",
        "print(y_train)\n",
        "\n",
        "print(\"\\nValidation data (X_val):\")\n",
        "print(X_val)\n",
        "print(\"\\nValidation labels (y_val):\")\n",
        "print(y_val)"
      ],
      "metadata": {
        "id": "t5G21ZFmPV6N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# T1) Plotting, graphs, visualization\n",
        "# Train model with checkpointing and L1_L2 regularization\n",
        "from tensorflow.keras.regularizers import l1_l2\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# U1)Use multi_layer_model instead of model\n",
        "checkpoint = ModelCheckpoint('best_model.h5', save_best_only=True, monitor='val_accuracy', mode='max')\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, mode='min')\n",
        "\n",
        "multi_layer_modelSS = Sequential()\n",
        "multi_layer_modelSS.add(Dense(16, activation='relu', input_dim=8, kernel_regularizer=l1_l2(l1=0.01, l2=0.01)))\n",
        "multi_layer_modelSS.add(Dense(8, activation='relu', kernel_regularizer=l1_l2(l1=0.01, l2=0.01)))\n",
        "multi_layer_modelSS.add(Dense(1, activation='sigmoid'))\n",
        "multi_layer_modelSS.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "history = multi_layer_modelSS.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=20, batch_size=32, callbacks=[checkpoint, early_stopping])\n"
      ],
      "metadata": {
        "id": "bRVBJPoOPZtf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# W1)Plot accuracy and loss\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model Accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model Loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "6mbtnYNlPdLQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# V1) Print accuracy and loss on both training and validation dataset\n",
        "_, train_accuracy = multi_layer_modelSS.evaluate(X_train, y_train, verbose=0)\n",
        "_, val_accuracy = multi_layer_modelSS.evaluate(X_val, y_val, verbose=0)\n",
        "print('Training Accuracy: %.2f' % (train_accuracy * 100))\n",
        "print('Validation Accuracy: %.2f' % (val_accuracy * 100))\n",
        "\n",
        "# W1) Print recall, precision, F1-score\n",
        "y_val_pred = (multi_layer_modelSS.predict(X_val) > 0.5).astype(int).flatten()\n",
        "\n",
        "true_positive = np.sum((y_val_pred == 1) & (y_val == 1))\n",
        "false_positive = np.sum((y_val_pred == 1) & (y_val == 0))\n",
        "false_negative = np.sum((y_val_pred == 0) & (y_val == 1))\n",
        "\n",
        "recall = true_positive / (true_positive + false_negative)\n",
        "precision = true_positive / (true_positive + false_positive)\n",
        "f1_score = 2 * precision * recall / (precision + recall)\n",
        "\n",
        "print('Recall: %.2f' % recall)\n",
        "print('Precision: %.2f' % precision)\n",
        "print('F1 Score: %.2f' % f1_score)\n"
      ],
      "metadata": {
        "id": "349H9xjjy-GK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# X1)Import required libraries\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Create and train a logistic regression model with increased max_iter\n",
        "log_reg_model = LogisticRegression(max_iter=1000)\n",
        "log_reg_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the validation set\n",
        "y_pred_val = log_reg_model.predict(X_val)\n",
        "\n",
        "# Calculate the accuracy on the validation set\n",
        "log_reg_val_accuracy = accuracy_score(y_val, y_pred_val)\n",
        "print('Logistic Regression Model Validation Accuracy: %.2f' % (log_reg_val_accuracy * 100))\n"
      ],
      "metadata": {
        "id": "MBy9OO-nfRG-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn.dummy import DummyClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Create and train a random baseline classifier\n",
        "random_baseline_model = DummyClassifier(strategy='stratified', random_state=42)\n",
        "random_baseline_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the validation set\n",
        "y_pred_val = random_baseline_model.predict(X_val)\n",
        "\n",
        "# Calculate the accuracy on the validation set\n",
        "random_baseline_val_accuracy = accuracy_score(y_val, y_pred_val)\n",
        "print('Random Baseline Classifier Validation Accuracy: %.2f' % (random_baseline_val_accuracy * 100))\n"
      ],
      "metadata": {
        "id": "vTkABoSygFUH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Store the accuracy scores\n",
        "accuracy_scores = {\n",
        "    'Multi-layer ModelSS': val_accuracy * 100,\n",
        "    'Logistic Regression Model': log_reg_val_accuracy * 100,\n",
        "    'Random Baseline Classifier': random_baseline_val_accuracy * 100\n",
        "}\n",
        "\n",
        "# Create a bar graph\n",
        "fig, ax = plt.subplots()\n",
        "bar_positions = range(len(accuracy_scores))\n",
        "bar_heights = accuracy_scores.values()\n",
        "\n",
        "bars = ax.bar(bar_positions, bar_heights)\n",
        "\n",
        "# Set the ticks and labels for the x-axis\n",
        "ax.set_xticks(bar_positions)\n",
        "ax.set_xticklabels(accuracy_scores.keys(), rotation=45)\n",
        "\n",
        "# Set the title and labels\n",
        "ax.set_title('Accuracy Comparison')\n",
        "ax.set_ylabel('Accuracy (%)')\n",
        "\n",
        "# Function to add percentage values on top of the bars\n",
        "def autolabel(rects):\n",
        "    for rect in rects:\n",
        "        height = rect.get_height()\n",
        "        ax.annotate(f'{height:.2f}%',\n",
        "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
        "                    xytext=(0, 3),  # 3 points vertical offset\n",
        "                    textcoords=\"offset points\",\n",
        "                    ha='center', va='bottom')\n",
        "\n",
        "\n",
        "# Call the function to add percentage values\n",
        "autolabel(bars)\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "pQFS5SpOu4Ao"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**PHASE 4 FEATURE REDUCTION**"
      ],
      "metadata": {
        "id": "MFj2cB1NJTkg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "WITH CHECKPOINTING"
      ],
      "metadata": {
        "id": "MR_8jlnD0g42"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Feature importance and model comparison\n",
        "num_features = X.shape[1]\n",
        "val_accuracies = []\n",
        "\n",
        "for i in range(num_features):\n",
        "    single_feature_model = Sequential()\n",
        "    single_feature_model.add(Dense(2, activation='relu', input_dim=1))\n",
        "    single_feature_model.add(Dense(1, activation='sigmoid'))\n",
        "    single_feature_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    \n",
        "    X_train_single = X_train[:, i].reshape(-1, 1)\n",
        "    X_val_single = X_val[:, i].reshape(-1, 1)\n",
        "\n",
        "    checkpoint = ModelCheckpoint(f'single_feature_model_{i}.h5', monitor='val_accuracy', save_best_only=True)\n",
        "    \n",
        "    single_feature_model.fit(X_train_single, y_train, epochs=20, batch_size=32, verbose=0,\n",
        "                             validation_data=(X_val_single, y_val), callbacks=[checkpoint])\n",
        "    \n",
        "    _, single_feature_val_accuracy = single_feature_model.evaluate(X_val_single, y_val, verbose=0)\n",
        "    val_accuracies.append(single_feature_val_accuracy)\n",
        "\n",
        "# Plot validation accuracies\n",
        "plt.bar(range(num_features), val_accuracies)\n",
        "plt.xlabel('Feature Index')\n",
        "plt.ylabel('Validation Accuracy')\n",
        "plt.ylim(0.8, 1)\n",
        "plt.title('Feature Importance')\n",
        "plt.show()\n",
        "\n",
        "# Remove unimportant features and compare\n",
        "sorted_indices = np.argsort(val_accuracies)\n",
        "val_accuracies_reduced_features = []\n",
        "\n",
        "for i in range(num_features):\n",
        "    reduced_features_indices = sorted_indices[:i]\n",
        "    X_train_reduced = X_train[:, reduced_features_indices]\n",
        "    X_val_reduced = X_val[:, reduced_features_indices]\n",
        "\n",
        "    reduced_features_model = Sequential()\n",
        "    reduced_features_model.add(Dense(2, activation='relu', input_dim=i))\n",
        "    reduced_features_model.add(Dense(1, activation='sigmoid'))\n",
        "    reduced_features_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    \n",
        "    checkpoint = ModelCheckpoint(f'reduced_features_model_{i}.h5', monitor='val_accuracy', save_best_only=True)\n",
        "\n",
        "    reduced_features_model.fit(X_train_reduced, y_train, epochs=20, batch_size=32, verbose=0,\n",
        "                               validation_data=(X_val_reduced, y_val), callbacks=[checkpoint])\n",
        "    \n",
        "    _, reduced_features_val_accuracy = reduced_features_model.evaluate(X_val_reduced, y_val, verbose=0)\n",
        "    val_accuracies_reduced_features.append(reduced_features_val_accuracy)\n",
        "\n",
        "# Plot validation accuracies\n",
        "plt.plot(val_accuracies_reduced_features)\n",
        "plt.xlabel('Number of Removed Features')\n",
        "plt.ylabel('Validation Accuracy')\n",
        "plt.title('Validation Accuracy vs. Removed Features')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "YUPXDmUFV40c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "WITHOUT CHECKPOINTING"
      ],
      "metadata": {
        "id": "T5oYRfRO0kn_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**PHASE 4 WITH SHAP FEATURE REDUCTION TECHNIQUE**"
      ],
      "metadata": {
        "id": "Xkgh02AAJK7U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lime\n",
        "!pip install shap\n"
      ],
      "metadata": {
        "id": "XpVx3NMxK1vB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PHASE 4\n",
        "# Z3)FEATURE SELECTION USING SHAP\n",
        "\n",
        "import shap\n",
        "\n",
        "# Z4) Feature importance using SHAP values\n",
        "# Train an XGBoost model\n",
        "import xgboost\n",
        "model = xgboost.train({\"learning_rate\": 0.01}, xgboost.DMatrix(X_train, label=y_train), 100)\n",
        "\n",
        "# Z5)Explain the model's predictions using SHAP values\n",
        "explainer = shap.Explainer(model)\n",
        "shap_values = explainer(X_train)\n",
        "\n",
        "# Z6)Plot the SHAP values for each feature\n",
        "shap.summary_plot(shap_values, X_train, plot_type=\"bar\")\n",
        "\n",
        "# Z7) Remove unimportant features and compare\n",
        "num_features = X_train.shape[1]\n",
        "sorted_indices = np.argsort(np.abs(shap_values.values).mean(0))[::-1]\n",
        "val_accuracies_reduced_features = []\n",
        "\n",
        "for i in range(num_features):\n",
        "    reduced_features_indices = sorted_indices[:i+1]\n",
        "    X_train_reduced = X_train[:, reduced_features_indices]\n",
        "    X_val_reduced = X_val[:, reduced_features_indices]\n",
        "\n",
        "    reduced_features_model = Sequential()\n",
        "    reduced_features_model.add(Dense(2, activation='relu', input_dim=i+1))\n",
        "    reduced_features_model.add(Dense(1, activation='sigmoid'))\n",
        "    reduced_features_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    reduced_features_model.fit(X_train_reduced, y_train, epochs=20, batch_size=32, verbose=0)\n",
        "\n",
        "    _, reduced_features_val_accuracy = reduced_features_model.evaluate(X_val_reduced, y_val, verbose=0)\n",
        "    val_accuracies_reduced_features.append(reduced_features_val_accuracy)\n",
        "\n",
        "# Z8)Plot validation accuracies\n",
        "plt.plot(val_accuracies_reduced_features)\n",
        "plt.xlabel('Number of Top Features')\n",
        "plt.ylabel('Validation Accuracy')\n",
        "plt.title('Validation Accuracy vs. Top Features')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "QCiALqVwJIrs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}